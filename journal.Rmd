---
title: "Data Science Fundamentals"
author: "Milena Roesch"
date: "09/04/2020"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

<style>
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: #2DC6D6;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE, cache.lazy = FALSE)
```
2020 | 9 | 3 Last compiled: `r Sys.Date()`

# Olist - Session 2 - Intro to the tidyverse

The first challenge of the course consisted of two parts:
1. Sales by location and year
2. Redoing the initial analysis using english translations to the product categories

## Sales by location and year

```{r}
# 1.0 Load libraries ---
library("tidyverse")
library(readr)

# 2.0 Importing files ---
sellers_tbl <- read_csv(file = "00_data/01_e-commerce/01_raw_data/olist_sellers_dataset.csv")
order_items_tbl <- read_csv(file = "00_data/01_e-commerce/01_raw_data/olist_order_items_dataset.csv")
orders_tbl      <- read_csv(file = "00_data/01_e-commerce/01_raw_data/olist_orders_dataset.csv")
# 3.0 Examining Data ---
glimpse(sellers_tbl)

# 4.0 Wrangling data
orders_sellers_joined_tbl <- sellers_tbl %>% 
                              left_join(order_items_tbl) %>%
                              left_join(orders_tbl)

orders_sellers_wrangled_tbl <- orders_sellers_joined_tbl %>%
  separate(col    = seller.location,
           into   = c("city.seller.location", "state.seller.location"),
           sep    = ", ",
           convert = T) %>%
  mutate(total.price = freight.value + price) %>%
  
  # Reorganize
  select(-shipping.limit.date, -order.approved.at) %>%
  select(-starts_with("product.")) %>%   #?ends_with --> select helpers
  select(-ends_with(".date")) %>%
  
  # Rename
  rename(order_date = order.purchase.timestamp) %>% 
  set_names(names(.) %>% str_replace_all("\\.", "_"))

#Sales by location and year
library(lubridate)

# Step 1 - Manipulate

revenue_by_loc_year_tbl <- orders_sellers_wrangled_tbl %>% 
  
  # Select columns
  select(order_date, total_price, state_seller_location) %>% 
  mutate(year = year(order_date)) %>% 
  
  # Filter  > 1.000.000
  group_by(state_seller_location) %>% 
  filter(sum(total_price) > 1000000) %>% 
  ungroup() %>% 
  
  # Group by and summarize year and main catgegory
  group_by(year, state_seller_location) %>% 
  summarise(revenue = sum(total_price)) %>% 
  ungroup() %>% 
  
  # Format $ Text
  mutate(revenue_text = scales::dollar(revenue))


# Step 2 - Visualize
revenue_by_loc_year_tbl %>% 
  
  # Set up x, y, fill
  ggplot(aes(x = year, y = revenue, fill = state_seller_location)) +
  
  # Geometries
  geom_col() +
  facet_wrap(~ state_seller_location) + 
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Revenue by year and location",
    subtitle = "Each location shows an upward trend",
    fill = "Location"
  )

```

## Initial analysis (exercise) using english translations to the product categories

As a continuation of the previous code (same script)
```{r}
# 1.0 Load libraries ----
#install.packages("readxl")
library("tidyverse")
library(readr)
library(readxl)
# 2.0 Importing Files ----
order_items_tbl <- read_csv(file = "00_data/01_e-commerce/01_raw_data/olist_order_items_dataset.csv") 
products_tbl    <- read_csv(file = "00_data/01_e-commerce/01_raw_data/olist_products_dataset.csv")
orders_tbl      <- read_csv(file = "00_data/01_e-commerce/01_raw_data/olist_orders_dataset.csv")
translation_prod_cat_tbl      <- read_excel("00_data/01_e-commerce/01_raw_data/product_category_name_translation.xlsx")

# 3.0 Examining Data ----
glimpse(translation_prod_cat_tbl)
products_tbl
glimpse(orders_tbl)

#Excurse: Rename
translation_prod_cat_changed_tbl <- translation_prod_cat_tbl %>%
  set_names(names(.) %>% str_replace_all("\\_", "."))

# 4.0 Joining Data ----
order_items_joined_tbl <- order_items_tbl %>%
  left_join(orders_tbl) %>%
  left_join(products_tbl)

order_items_engl_joined_tbl <- order_items_joined_tbl %>%
  left_join(translation_prod_cat_changed_tbl)

# 5.0 Wrangling Data ----
order_items_engl_wrangled_tbl <- order_items_engl_joined_tbl %>%
  separate(col    = product.category.name.english,
           into   = c("main.category.name", "sub.category.name"),
           sep    = " - ",
           remove = FALSE) %>%
  mutate(total.price = price + freight.value) %>%
  select(-product.category.name, -shipping.limit.date, order.approved.at) %>%
  select(-starts_with("product.")) %>%
  select(-ends_with(".date")) %>%
  bind_cols(order_items_engl_joined_tbl %>% select(product.id)) %>%
  select(contains("timestamp"), contains(".id"),
         main.category.name, sub.category.name, price, freight.value, total.price,
         everything()) %>% 
  rename(order_date = order.purchase.timestamp) %>% 
  set_names(names(.) %>% str_replace_all("\\.", "_"))

# 6.0 Business Insights ----
# 6.1 Sales by Year ----

# Step 1 - Manipulate

library(lubridate)

revenue_by_year_engl_tbl <- order_items_engl_wrangled_tbl %>%
  select(order_date, total_price) %>%
  mutate(year = year(order_date)) %>%
  group_by(year) %>% 
  summarize(revenue = sum(total_price)) %>%
  mutate(revenue_text = scales::dollar(revenue))


# Step 2 - Visualize
revenue_by_year_engl_tbl %>%
  ggplot(aes(x = year, y = revenue)) +
  geom_col(fill = "#2DC6D6") + # Use geom_col for a bar plot
  geom_label(aes(label = revenue_text)) + # Adding labels to the bars
  geom_smooth(method = "lm", se = FALSE) + # Adding a trendline
  scale_y_continuous(labels = scales::dollar) + # Change the y-axis
  labs(
    title    = "Revenue by year",
    subtitle = "Upward Trend",
    x = "", # Override defaults for x and y
    y = "Revenue"
  )

# 6.2 Sales by Year and Category 2 ----

# Step 1 - Manipulate
revenue_by_year_engl_cat_main_tbl <- order_items_engl_wrangled_tbl %>%
  select(order_date, total_price, main_category_name) %>% 
  mutate(year = year(order_date)) %>%
  group_by(main_category_name) %>% 
  filter(sum(total_price) > 1000000) %>% # If you run the code up here, R will tell you that we have 6 groups
  ungroup() %>%
  group_by(year, main_category_name) %>% 
  summarise(revenue = sum(total_price)) %>% 
  ungroup() %>%
  mutate(revenue_text = scales::dollar(revenue))
# Step 2 - Visualize
revenue_by_year_engl_cat_main_tbl %>%
  ggplot(aes(x = year, y = revenue, fill = main_category_name)) +
  geom_col() + # Run up to here to get a stacked bar plot
  facet_wrap(~ main_category_name) +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Revenue by year and main category",
    subtitle = "Each product category has an upward trend",
    fill = "Main category" 
  )

# 7.0 Writing Files ---- --> blended out since there is no need to save it for the purpose of the journal

#install.packages("fs")
#library(fs)
#fs::dir_create("00_data/01_e-commerce/04_wrangled_data_student/challenge1")

# 7.1 Excel ----
#install.packages("writexl")
#library("writexl")
#write_xlsx(order_items_wrangled_tbl, "00_data/01_e-commerce/04_wrangled_data_student/challenge1/order_items.xlsx")

# 7.2 CSV ----
#write.csv(order_items_wrangled_tbl, "00_data/01_e-commerce/04_wrangled_data_student/challenge1/order_items.csv")
# 7.3 RDS ----
#saveRDS(order_items_wrangled_tbl, "00_data/01_e-commerce/04_wrangled_data_student/challenge1/order_items.rds")

```

# Data Scraping from an API and a bike website - Session 3 - Data Acquisition
## Scraping Twitter Data
```{r}
#install.packages("twitteR") #install package
library(twitteR) #load package

#consumer_key <- #not able to be shown on My Journal Website due to security reasons
#consumer_secret <- #not able to be shown on My Journal Website due to security reasons 
#access_token <- #not able to be shown on My Journal Website due to security reasons
#access_secret <- #not able to be shown on My Journal Website due to security reasons

#setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

#virus <- searchTwitter('#wendlergate', n = 200, since = '2020-09-01', retryOnRateLimit = 1e3)
#virus_df = twListToDF(virus)
```


## Webscraping 
```{r}
# WEBSCRAPING ----

# 1.0 LIBRARIES ----

library(tidyverse) # Main Package - Loads dplyr, purrr
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
library(stringr)
library(RSQLite)

# 2.0 OPEN URL ----
url_home          <- "https://www.radon-bikes.de/roadbike/carbon/bikegrid/"
xopen(url_home) # Open links directly from RStudio to inspect them

# Read in the HTML for category of carbon bikes
html_home         <- read_html(url_home)

# 3.0 WEB SCRAPING ----
#Web scrape the product names
product_name_tbl <- html_home %>%
  #Get the nodes for the product names ...
  html_nodes(css = ".m-bikegrid__info > a > div > h4") %>%
  # ...and extract the text of the html attribute
  html_text() %>%
  # Delete the unnecessary formatting
  str_replace_all("[\t\n\r\v\f]" , "") %>%
  # Delete whitespace
  trimws() %>%
  # Convert vector to tibble
  enframe(name = "position", value = "product_name")

#Web scrape the product prices
product_price_tbl <- html_home %>%
  #Get the nodes for the product prices ...
  html_nodes(css = ".currency_eur > .m-bikegrid__price--active") %>%
  # ...and extract the information of the span attribute
  html_text("span") %>%
  # Convert vector to tibble
  enframe(name = "position", value = "product_price")

# 4.0 DATABASE  ----
# Create a database
conn <- RSQLite::dbConnect(drv    = SQLite(), 
                          dbname = "00_data/03_bike_shop/Radon_carbon_bikes.sqlite")
# Write tables to database
dbWriteTable(conn, "bike_names", product_name_tbl, overwrite=TRUE)
dbWriteTable(conn, "bike_prices", product_price_tbl, overwrite=TRUE)

# List tables of database 
# dbListTables(conn)

# Get price and respective product Name from database
dbGetQuery(conn, "SELECT product_name, product_price FROM bike_names, bike_prices WHERE bike_names.position = bike_prices.position")
dbDisconnect(conn)
```

# Data Wrangling with Patent Data - Session 4 - Data Wrangling
```{r}
# 1.0 LIBRARIES ----
#install.packages("data.table")
#install.packages("vroom")
#install.packages("tictoc")


# Tidyverse
library(tidyverse)
library(vroom)

# Data Table
library(data.table)

# Counter
library(tictoc)

# Importing Data ------
# assignee
col_types <- list(
  id = col_character(),
  type = col_skip(),
  name_first = col_skip(),
  name_last = col_skip(),
  organization = col_character()
)

assignee_tbl <- vroom(
  file       = "00_data/04_patents/assignee.tsv", 
  delim      = "\t", 
  col_names = names(col_types),
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

setDT(assignee_tbl)

# patent assignee
col_types <- list(
  patent_id = col_character(),
  assignee_id = col_character(),
  location_id = col_skip()
)

patent_assignee_tbl <- vroom(
  file       = "00_data/04_patents/patent_assignee.tsv", 
  delim      = "\t", 
  col_names = names(col_types),
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

setDT(patent_assignee_tbl)

# patent
col_types <- list(
  id = col_character(),
  type = col_skip(),
  number = col_skip(),
  country = col_skip(),
  date = col_date("%Y-%m-%d"),
  abstract = col_skip(),
  title = col_skip(),
  kind = col_skip(),
  num_claims = col_skip(),
  filename = col_skip(),
  withdrawn = col_skip()
 )
 
 patent_tbl <- vroom(
   file       = "00_data/04_patents/patent.tsv", 
   delim      = "\t", 
   col_names  = names(col_types),
   col_types  = col_types,
   na         = c("", "NA", "NULL")
 )
 
 setDT(patent_tbl)
 
# uspc
col_types <- list(
  uuid = col_skip(),
  patent_id = col_character(),
  mainclass_id = col_character(),
  subclass_id = col_skip(),
  sequence = col_skip()
)

uspc_tbl <- vroom(
  file       = "00_data/04_patents/uspc.tsv",
  delim      = "\t",
  col_names  = names(col_types),
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

setDT(uspc_tbl)

# mainclass description
col_types <- list(
  id = col_character(),
  title = col_character())

mainclass_current_tbl <- vroom(
  file       = "00_data/04_patents/mainclass_current.tsv", 
  delim      = "\t", 
  col_names  = names(col_types),
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
setDT(mainclass_current_tbl)

# Change of column names to avoid misalignment

setnames(patent_assignee_tbl, "assignee_id", "id")
setnames(patent_tbl, "id", "patent_id")
setnames(mainclass_current_tbl, "id", "mainclass_id")

# Joining tables -----
# Combine assignee_tbl with patent_assignee_tbl
combined_patent_patent_assignee_tbl <- merge(x = assignee_tbl, y = patent_assignee_tbl, 
                       by    = "id", 
                       all.x = TRUE, 
                       all.y = FALSE)
setDT(combined_patent_patent_assignee_tbl)
rm(assignee_tbl, patent_assignee_tbl)
gc()

# Combine result with patent_tbl
combined_patent_patent_assignee_patent_tbl <- merge(x = combined_patent_patent_assignee_tbl, y = patent_tbl, 
                                             by    = "patent_id", 
                                             all.x = TRUE, 
                                             all.y = FALSE)
setDT(combined_patent_patent_assignee_patent_tbl)
rm(combined_patent_patent_assignee_tbl, patent_tbl)
gc()

# Combine result with uspc_tbl
combined_patent_patent_assignee_patent_uspc_tbl <- merge(x = combined_patent_patent_assignee_patent_tbl, y = uspc_tbl, 
                                                    by    = "patent_id", 
                                                    all.x = TRUE, 
                                                    all.y = FALSE)
setDT(combined_patent_patent_assignee_patent_uspc_tbl)
rm(combined_patent_patent_assignee_patent_tbl, uspc_tbl)
gc()

# Combine result with mainclass_current_tbl
combined_patent_data_tbl <- merge(x = combined_patent_patent_assignee_patent_uspc_tbl, y = mainclass_current_tbl, 
                                                         by    = "mainclass_id", 
                                                         all.x = TRUE, 
                                                         all.y = FALSE)
setDT(combined_patent_data_tbl)
rm(combined_patent_patent_assignee_patent_uspc_tbl, mainclass_current_tbl)
gc()

# Preparing the data table  -----

setkey(combined_patent_data_tbl, "organization", "date", "mainclass_id")
key(combined_patent_data_tbl)

?setorder()
setorderv(combined_patent_data_tbl, c("organization", "date", "mainclass_id"))

# 1 - Top 10 US organizations with the most patents -----
combined_patent_data_tbl %>%
  group_by(id)%>%
  filter(!is.na(organization))%>%
  count(organization, sort = TRUE)%>%
  head(n = 10)

# 2 - Top 10 US organizations with the most patents in 2019 ----
combined_patent_data_tbl %>%
  group_by(id)%>%
  filter(!is.na(organization))%>%
  filter(year(date) == 2019)%>%
  count(organization, sort = TRUE)%>%
  head(n = 10)

# 3 - Top 5 USPTO Classes of top 10 organizations ----
combined_patent_data_tbl %>%
  select(id, organization, mainclass_id, title)%>%
  group_by(id)%>%
  filter(!is.na(organization))%>%
  filter(!is.na(mainclass_id))%>%
  filter(title != "DOES NOT EXIST") %>%
  filter(title != "unclassified") %>%
  count(organization, title, sort = TRUE)%>%
  head(n = 10)
```

# Data Visualization of COVID Data - Session 5 - Data Visualization
```{r}
library(tidyverse)
library(lubridate)
library(maps)
library(scales)

covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")

# Challenge 1 -----------------
# Format data ----
cumulative_cases_by_day_tbl <- covid_data_tbl %>%
  select(dateRep, cases, countriesAndTerritories) %>%
  filter(countriesAndTerritories %in% c("Germany","United_Kingdom", "France", "Spain", "United_States_of_America")) %>%

  # Grouping by country & date, and cumulatively summarize number of cases
  group_by(countriesAndTerritories) %>%
  mutate(dateRep = as.Date(dateRep, "%d/%m/%Y")) %>%
  arrange(dateRep) %>%
  mutate(cumulative_cases = cumsum(cases)) %>%
  ungroup()

#cumulative_cases_by_day_tbl

# Plot ----
cumulative_cases_by_day_tbl %>%
  ggplot(aes(x = dateRep, y = cumulative_cases, color = countriesAndTerritories)) +
  geom_line(size = 1) +
  scale_color_brewer(palette = "Accent") +
  labs(
    title = "COVID-19 confirmed cases worldwide",
    subtitle = "As of 11/2020",
    x = "Year 2020",
    y = "Confirmed CoVID-19 cases",
    color = "Country"
  ) +
  theme_gray() +
  theme(legend.position  = "right",
        legend.direction = "vertical",
        plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45)) +
  scale_x_date(
    date_breaks = "1 month", date_labels = "%B",
    date_minor_breaks = "1 month"
  ) +
  scale_y_continuous(
    breaks = seq(0, 15000000, by = 2500000),
    labels = scales::label_number_si(accuracy = 0.1),
    minor_breaks = seq(0, 10000000, by = 1250000)
  )

# Challenge 2 ----------
world <- map_data("world")
cumulative_deaths_per_country_tbl <- covid_data_tbl %>%
  select(popData2019, deaths, countriesAndTerritories) %>%
  group_by(countriesAndTerritories) %>% 
  summarise(total_deaths = sum(deaths), population_2019 = median(popData2019)) %>%
  ungroup() %>%
  mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
  mutate(countriesAndTerritories = case_when(
    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories
  )) %>%
  mutate (mortality_rate = total_deaths/population_2019) %>%
  rename("region" = "countriesAndTerritories")

world_mortality_rate_covid <- left_join(world, cumulative_deaths_per_country_tbl)
# 
ggplot(world_mortality_rate_covid) +
  geom_map(aes(map_id = region, fill = mortality_rate), map = world_mortality_rate_covid, data = world_mortality_rate_covid) +
  expand_limits(x = world_mortality_rate_covid$long, y = world_mortality_rate_covid$lat) +
  labs(
        title = "Mortality rate of COVID-19 worldwide",
        subtitle = "As of 11/2020",
        x = NULL,
        y = NULL,
        fill = "Mortality Rate"
      ) +
  theme(
    axis.text = element_blank(),
    axis.line = element_blank(),
    axis.ticks = element_blank(),
    panel.border = element_blank(),
    panel.grid = element_blank(),
    axis.title = element_blank()
  ) +
  scale_fill_gradient(
    low = "#D80000",
    high = "#3F0000",
    space = "Lab",
    na.value = "grey50",
    guide = "colourbar",
    aesthetics = "fill",
    breaks = seq(0.0000, 0.0012, by=0.0003), 
    labels = scales::percent_format(accuracy = 0.001)
    )
```
